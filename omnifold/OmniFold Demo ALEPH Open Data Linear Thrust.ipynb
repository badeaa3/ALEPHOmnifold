{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmniFold with ALEPH Open Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anthonybadea/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "import omnifold\n",
    "import modplot\n",
    "import ibu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,4)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleph_path = '/Users/anthonybadea/Documents/ALEPH/ALEPH'\n",
    "event_selections = [\n",
    "    'passesNTupleAfterCut',\n",
    "    'passesTotalChgEnergyMin', \n",
    "    'passesNTrkMin', \n",
    "    'passesNeuNch',\n",
    "    'passesSTheta'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_years = [\n",
    "    #'1992', \n",
    "    #'1993', \n",
    "    '1994', \n",
    "    #'1995'\n",
    "]\n",
    "\n",
    "# load data by year\n",
    "data = {'yearly_thrust': [], 'yearly_evmask': []}\n",
    "for year in data_years:\n",
    "    data_file = uproot.open(os.path.join(aleph_path, 'LEP1Data{}_recons_aftercut-MERGED.root'.format(year)))\n",
    "    t = data_file['t']\n",
    "    event_mask = np.ones(t.num_entries, dtype=bool) # len(t['EventNo'])\n",
    "    for evsel in event_selections:\n",
    "        event_mask = np.logical_and(event_mask,t[evsel].array()) #&= t[evsel].array()\n",
    "        \n",
    "    data['yearly_evmask'].append(event_mask)\n",
    "    data['yearly_thrust'].append(1 - t['Thrust'].array())\n",
    "\n",
    "data['evmask'] = np.concatenate(data['yearly_evmask'])\n",
    "data['thrust'] = np.concatenate(data['yearly_thrust'])\n",
    "data['sel_thrust'] = data['thrust'][data['evmask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mc, currently just get thrust values\n",
    "mc_file = uproot.open(os.path.join(aleph_path, 'alephMCRecoAfterCutPaths_1994.root'))\n",
    "\n",
    "event_mask = np.ones(mc_file['t'].num_entries, dtype=bool) #len(mc_file['t']['EventNo']\n",
    "before_masks = []\n",
    "for evsel in event_selections:\n",
    "    event_mask = np.logical_and(event_mask,mc_file['t'][evsel].array()) #&=mc_file['t'][evsel].array()\n",
    "    before_masks.append(mc_file['tgenBefore'][evsel].array())\n",
    "    \n",
    "mc = {\n",
    "    'sim_thrust': 1 - mc_file['t']['Thrust'].array(),\n",
    "    'gen_thrust': 1 - mc_file['tgen']['Thrust'].array(),\n",
    "    'genBefore_thrust': 1 - mc_file['tgenBefore']['Thrust'].array(),\n",
    "}\n",
    "mc['sel_sim_thrust'] = mc['sim_thrust'][event_mask]\n",
    "mc['sel_gen_thrust'] = mc['gen_thrust'][event_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these dicts are used by OmniFold\n",
    "nature = {'data_thrust': data['sel_thrust']}\n",
    "synthetic = {'sim_thrust': mc['sel_sim_thrust'], 'gen_thrust': mc['sel_gen_thrust']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the distributions to determine the bin-by-bin corrections\n",
    "bins = np.linspace(0, 0.5, 51)\n",
    "density = True\n",
    "\n",
    "opts = {'bins': bins, 'histtype': 'step', 'density': density}\n",
    "\n",
    "plt.hist(data['thrust'], color='black', label='Data - 1994', **opts)\n",
    "plt.hist(data['sel_thrust'], color='gray', label='Data - 1994 sel.', **opts)\n",
    "\n",
    "plt.hist(mc['sim_thrust'], color='orange', label='SIM', **opts)\n",
    "simhist = plt.hist(synthetic['sim_thrust'], color='tab:olive', label='SIM sel.', **opts)[0]\n",
    "\n",
    "plt.hist(mc['gen_thrust'], color='blue', label='GEN', **opts)\n",
    "genhist = plt.hist(synthetic['gen_thrust'], color='cyan', label='GEN sel.', **opts)[0]\n",
    "genbhist = plt.hist(mc['genBefore_thrust'], color='green', label='GEN before', **opts)[0]\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "#plt.ylim(10**-5.5, 1)\n",
    "\n",
    "plt.xlabel(r'Thrust $1-T$')\n",
    "plt.ylabel('Probability Density')\n",
    "\n",
    "plt.legend(loc=(0.1, 0.025), frameon=False)\n",
    "\n",
    "plt.savefig('../plots/ThrustDistributions.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Unfolding Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many iterations of the unfolding process\n",
    "itnum = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the observables to use for multifold (a single one just indicates unifold)\n",
    "obs_multifold = ['Thrust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to hold information about the observables\n",
    "obs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the thrust and histogram style information\n",
    "obs.setdefault('Thrust', {}).update({\n",
    "    'func': lambda dset, ptype: dset[ptype + '_thrust'],\n",
    "    'nbins_det': 50, 'nbins_mc': 50,\n",
    "    'yscale': 'log',\n",
    "    'xlim': (0, 0.5), 'ylim': (10**-4.5, 100), 'ylim_ratio': (0.7, 1.3),\n",
    "    'ytick_ratio_step': 0.15,\n",
    "    'xlabel': r'Thrust $1-T$', 'symbol': r'$1-T$',\n",
    "    'ylabel': r'Normalized Cross Section', 'ylabel_ratio': 'Ratio to\\nGen.',\n",
    "    'stamp_xy': (0.5, 0.9),\n",
    "    'legend_loc': 'lower left', 'legend_ncol': 1\n",
    "})\n",
    "\n",
    "#obs.setdefault('LogThrust', {}).update({\n",
    "#    'func': lambda dset, ptype: dset[ptype + '_thrust'],\n",
    "#    'nbins_det': 50, 'nbins_mc': 50,\n",
    "#    'yscale': 'log',\n",
    "#    'xlim': (-8, np.log(0.5)), 'ylim': (10**-5.5, 1), 'ylim_ratio': (0.7, 1.3),\n",
    "#    'ytick_ratio_step': 0.15,\n",
    "#    'xlabel': r'Thrust $\\ln(1-T)$', 'symbol': r'$\\ln(1-T)$',\n",
    "#    'ylabel': r'Normalized Cross Section', 'ylabel_ratio': 'Ratio to\\nGen.',\n",
    "#    'stamp_xy': (0.5, 0.9),\n",
    "#    'legend_loc': 'lower left', 'legend_ncol': 1\n",
    "#})\n",
    "\n",
    "# additional histogram and plot style information\n",
    "hist_style = {'histtype': 'step', 'density': True, 'lw': 1, 'zorder': 2}\n",
    "gen_style = {'linestyle': '--', 'color': 'blue', 'lw': 1.15, 'label': r'\\textsc{Pythia} 6 Gen.'}\n",
    "truth_style = {'step': 'mid', 'edgecolor': 'green', 'facecolor': (0.75, 0.875, 0.75),\n",
    "               'lw': 1.25, 'zorder': 0, 'label': 'ALEPH Measurement'}\n",
    "ibu_style = {'ls': '-', 'marker': 'o', 'ms': 2.5, 'color': 'gray', 'zorder': 1}\n",
    "omnifold_style = {'ls': '-', 'marker': 's', 'ms': 2.5, 'color': 'tab:red', 'zorder': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Thrust\n"
     ]
    }
   ],
   "source": [
    "# calculate quantities to be stored in obs\n",
    "for obkey,ob in obs.items():\n",
    "    \n",
    "    # calculate observable for GEN, SIM, DATA, and TRUE\n",
    "    ob['genobs'], ob['simobs'] = ob['func'](synthetic, 'gen'), ob['func'](synthetic, 'sim')\n",
    "    ob['dataobs'] = ob['func'](nature, 'data')\n",
    "    \n",
    "    # setup bins\n",
    "    ob['bins_det'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_det']+1)\n",
    "    ob['bins_mc'] = np.linspace(ob['xlim'][0], ob['xlim'][1], ob['nbins_mc']+1)\n",
    "    ob['midbins_det'] = (ob['bins_det'][:-1] + ob['bins_det'][1:])/2\n",
    "    ob['midbins_mc'] = (ob['bins_mc'][:-1] + ob['bins_mc'][1:])/2\n",
    "    ob['binwidth_det'] = ob['bins_det'][1] - ob['bins_det'][0]\n",
    "    ob['binwidth_mc'] = ob['bins_mc'][1] - ob['bins_mc'][0]\n",
    "    \n",
    "    # get the histograms of GEN, DATA, and TRUTH level observables\n",
    "    ob['genobs_hist'] = np.histogram(ob['genobs'], bins=ob['bins_mc'], density=True)[0]\n",
    "    ob['data_hist'] = np.histogram(ob['dataobs'], bins=ob['bins_det'], density=True)[0]\n",
    "\n",
    "    # compute (and normalize) the response matrix between GEN and SIM\n",
    "    ob['response'] = np.histogram2d(ob['simobs'], ob['genobs'], bins=(ob['bins_det'], ob['bins_mc']))[0]\n",
    "    ob['response'] /= (ob['response'].sum(axis=0) + 10**-50)\n",
    "    \n",
    "    # perform iterative Bayesian unfolding\n",
    "    ob['ibu_phis'] = ibu.ibu(ob['data_hist'], ob['response'], ob['genobs_hist'], \n",
    "                             ob['binwidth_det'], ob['binwidth_mc'], it=itnum)\n",
    "    ob['ibu_phi_unc'] = ibu.ibu_unc(ob, it=itnum, nresamples=50)\n",
    "    \n",
    "    print('Done with', obkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OmniFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_layer_sizes = [100, 100, 100]\n",
    "val = 0.2\n",
    "\n",
    "# set up the array of data/simulation detector-level observables\n",
    "ob0 = obs_multifold[0]\n",
    "X_det = np.asarray([np.concatenate((obs[obkey]['dataobs'], obs[obkey]['simobs'])) for obkey in obs_multifold]).T\n",
    "Y_det = ef.utils.to_categorical(np.concatenate((np.ones(len(obs[ob0]['dataobs'])), \n",
    "                                                np.zeros(len(obs[ob0]['simobs'])))))\n",
    "\n",
    "# set up the array of generation particle-level observables\n",
    "X_gen = np.asarray([np.concatenate((obs[obkey]['genobs'], obs[obkey]['genobs'])) for obkey in obs_multifold]).T\n",
    "Y_gen = ef.utils.to_categorical(np.concatenate((np.ones(len(obs[ob0]['genobs'])), \n",
    "                                                np.zeros(len(obs[ob0]['genobs'])))))\n",
    "\n",
    "# standardize the inputs\n",
    "X_det = (X_det - np.mean(X_det, axis=0))/np.std(X_det, axis=0)\n",
    "X_gen = (X_gen - np.mean(X_gen, axis=0))/np.std(X_gen, axis=0)\n",
    "\n",
    "# reweight the sim and data to have the same total weight to begin with\n",
    "ndata, nsim = np.count_nonzero(Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
    "wdata = np.ones(ndata)\n",
    "winit = ndata/nsim*np.ones(nsim)\n",
    "\n",
    "# initialize the truth weights to the prior\n",
    "ws = [winit]\n",
    "\n",
    "# get permutation for det\n",
    "perm_det = np.random.permutation(len(winit) + len(wdata))\n",
    "invperm_det = np.argsort(perm_det)\n",
    "nval_det = int(val*len(perm_det))\n",
    "X_det_train, X_det_val = X_det[perm_det[:-nval_det]], X_det[perm_det[-nval_det:]]\n",
    "Y_det_train, Y_det_val = Y_det[perm_det[:-nval_det]], Y_det[perm_det[-nval_det:]]\n",
    "del X_det, Y_det\n",
    "\n",
    "# get an initial permutation for gen and duplicate (offset) it\n",
    "nval = int(val*len(winit))\n",
    "baseperm0 = np.random.permutation(len(winit))\n",
    "baseperm1 = baseperm0 + len(winit)\n",
    "\n",
    "# training examples are at beginning, val at end\n",
    "# concatenate into single train and val perms (shuffle each)\n",
    "trainperm = np.concatenate((baseperm0[:-nval], baseperm1[:-nval]))\n",
    "valperm = np.concatenate((baseperm0[-nval:], baseperm1[-nval:]))\n",
    "np.random.shuffle(trainperm)\n",
    "np.random.shuffle(valperm)\n",
    "\n",
    "# get final permutation for gen (ensured that the same events end up in val)\n",
    "perm_gen = np.concatenate((trainperm, valperm))\n",
    "invperm_gen = np.argsort(perm_gen)\n",
    "nval_gen = 2*nval\n",
    "X_gen_train, X_gen_val = X_gen[perm_gen[:-nval_gen]], X_gen[perm_gen[-nval_gen:]]\n",
    "Y_gen_train, Y_gen_val = Y_gen[perm_gen[:-nval_gen]], Y_gen[perm_gen[-nval_gen:]]\n",
    "del X_gen, Y_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCTR, reweights positive distribution to negative distribution\n",
    "# X: features\n",
    "# Y: categorical labels\n",
    "# model: model with fit/predict\n",
    "# fitargs: model fit arguments\n",
    "def reweight(X, Y, w, model, filepath, fitargs, val_data=None):\n",
    "\n",
    "    val_dict = {'validation_data': val_data} if val_data is not None else {}\n",
    "    model.fit(X, Y, sample_weight=w, **fitargs, **val_dict)\n",
    "    model.save_weights(filepath)\n",
    "    preds = model.predict(X, batch_size=10000)[:,1]\n",
    "    \n",
    "    # concatenate validation predictions into training predictions\n",
    "    if val_data is not None:\n",
    "        preds_val = model.predict(val_data[0], batch_size=10000)[:,1]\n",
    "        preds = np.concatenate((preds, preds_val))\n",
    "        w = np.concatenate((w, val_data[2]))\n",
    "\n",
    "    w *= preds/(1 - preds + 10**-50)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfolding iteration 0\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonybadea/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonybadea/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662570/1662570 - 5s - loss: 0.8867 - acc: 0.5139 - val_loss: 0.8848 - val_acc: 0.5936 - 5s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 5s - loss: 0.8853 - acc: 0.5153 - val_loss: 0.8860 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.5322 - val_loss: 0.8847 - val_acc: 0.5740 - 5s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.4991 - val_loss: 0.8848 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.4869 - val_loss: 0.8848 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.5035 - val_loss: 0.8848 - val_acc: 0.3707 - 5s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.4875 - val_loss: 0.8848 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.4842 - val_loss: 0.8848 - val_acc: 0.3717 - 5s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.5092 - val_loss: 0.8848 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.5016 - val_loss: 0.8848 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.5151 - val_loss: 0.8848 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.4973 - val_loss: 0.8848 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "1662570/1662570 - 5s - loss: 0.8848 - acc: 0.5150 - val_loss: 0.8848 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonybadea/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2276 - acc: 0.4996 - val_loss: 1.2265 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2264 - acc: 0.4997 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4996 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.4994 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.4997 - val_loss: 1.2259 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.5008 - val_loss: 1.2259 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.4999 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.4996 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.5000 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.4997 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.4996 - val_loss: 1.2258 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.5008 - val_loss: 1.2259 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.5000 - val_loss: 1.2259 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 00013: early stopping\n",
      "Unfolding iteration 1\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-1_Iteration-0\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-2_Iteration-0\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5472 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5493 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5570 - val_loss: 0.8828 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5658 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5593 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5725 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5913 - val_loss: 0.8828 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5797 - val_loss: 0.8828 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5719 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5750 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5767 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5543 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5641 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5808 - val_loss: 0.8828 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5642 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5642 - val_loss: 0.8828 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5497 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5832 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 19/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5542 - val_loss: 0.8828 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 20/100\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5759 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 21/100\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "1662570/1662570 - 5s - loss: 0.8828 - acc: 0.5629 - val_loss: 0.8828 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 00021: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5000 - val_loss: 1.2226 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.4998 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.4999 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.4995 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.4996 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.5004 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.5000 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.5006 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.5003 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.4997 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1202978/1202978 - 4s - loss: 1.2227 - acc: 0.4996 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 00011: early stopping\n",
      "Unfolding iteration 2\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-1_Iteration-1\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-2_Iteration-1\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6197 - val_loss: 0.8795 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6194 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 6s - loss: 0.8794 - acc: 0.6304 - val_loss: 0.8794 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 6s - loss: 0.8794 - acc: 0.6273 - val_loss: 0.8794 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6172 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6348 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6353 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6279 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6356 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6284 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6365 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6304 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6228 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6265 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6364 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "1662570/1662570 - 5s - loss: 0.8794 - acc: 0.6329 - val_loss: 0.8794 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 00016: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5000 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5000 - val_loss: 1.2228 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5005 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.4998 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.4997 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5005 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5010 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.4998 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.4992 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5005 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1202978/1202978 - 4s - loss: 1.2228 - acc: 0.5007 - val_loss: 1.2227 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: early stopping\n",
      "Unfolding iteration 3\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-1_Iteration-2\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-2_Iteration-2\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4754 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4504 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4719 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4412 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.4666 - val_loss: 0.8859 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4501 - val_loss: 0.8859 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4681 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4570 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.4460 - val_loss: 0.8859 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.4452 - val_loss: 0.8859 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4513 - val_loss: 0.8859 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.4549 - val_loss: 0.8859 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4544 - val_loss: 0.8859 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.4645 - val_loss: 0.8859 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 15/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4511 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4579 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4506 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4461 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 19/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4542 - val_loss: 0.8859 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 20/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4785 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 21/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.4372 - val_loss: 0.8859 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 22/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4850 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 23/100\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4447 - val_loss: 0.8859 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 24/100\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "1662570/1662570 - 5s - loss: 0.8859 - acc: 0.4562 - val_loss: 0.8859 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 00024: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5002 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5000 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4997 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4996 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4998 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4996 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5002 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4996 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4995 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5001 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5002 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4987 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5001 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4999 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.4991 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "1202978/1202978 - 4s - loss: 1.2233 - acc: 0.5000 - val_loss: 1.2233 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 00016: early stopping\n",
      "Unfolding iteration 4\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-1_Iteration-3\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw0_Step-2_Iteration-3\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 6s - loss: 0.8854 - acc: 0.4718 - val_loss: 0.8854 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4732 - val_loss: 0.8854 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4705 - val_loss: 0.8854 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4774 - val_loss: 0.8854 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4694 - val_loss: 0.8854 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4560 - val_loss: 0.8854 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4573 - val_loss: 0.8854 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4771 - val_loss: 0.8854 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4797 - val_loss: 0.8855 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8854 - acc: 0.4782 - val_loss: 0.8854 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4554 - val_loss: 0.8855 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4960 - val_loss: 0.8855 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 6s - loss: 0.8854 - acc: 0.4815 - val_loss: 0.8854 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 6s - loss: 0.8854 - acc: 0.4716 - val_loss: 0.8854 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1662570/1662570 - 6s - loss: 0.8854 - acc: 0.4623 - val_loss: 0.8855 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4779 - val_loss: 0.8855 - val_acc: 0.6382 - 5s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4833 - val_loss: 0.8855 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4671 - val_loss: 0.8854 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 19/100\n",
      "1662570/1662570 - 6s - loss: 0.8854 - acc: 0.4617 - val_loss: 0.8854 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 20/100\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "1662570/1662570 - 5s - loss: 0.8854 - acc: 0.4620 - val_loss: 0.8854 - val_acc: 0.3618 - 5s/epoch - 3us/sample\n",
      "Epoch 00020: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4998 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5001 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5005 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4998 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5003 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5005 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4998 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4997 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5004 - val_loss: 1.2231 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5005 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4992 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4998 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.5005 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4998 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4996 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1202978/1202978 - 4s - loss: 1.2230 - acc: 0.4999 - val_loss: 1.2230 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "1202978/1202978 - 5s - loss: 1.2230 - acc: 0.5005 - val_loss: 1.2230 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 00017: early stopping\n",
      "Unfolding iteration 0\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 7s - loss: 0.8880 - acc: 0.5060 - val_loss: 0.8851 - val_acc: 0.5156 - 7s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 6s - loss: 0.8859 - acc: 0.5171 - val_loss: 0.8852 - val_acc: 0.5174 - 6s/epoch - 4us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.5133 - val_loss: 0.8852 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4848 - val_loss: 0.8852 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.5086 - val_loss: 0.8852 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4915 - val_loss: 0.8852 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4858 - val_loss: 0.8852 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4907 - val_loss: 0.8852 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4831 - val_loss: 0.8852 - val_acc: 0.3724 - 6s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4911 - val_loss: 0.8852 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "1662570/1662570 - 6s - loss: 0.8852 - acc: 0.4927 - val_loss: 0.8852 - val_acc: 0.5527 - 6s/epoch - 3us/sample\n",
      "Epoch 00011: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2280 - acc: 0.5004 - val_loss: 1.2267 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2266 - acc: 0.5000 - val_loss: 1.2259 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2259 - acc: 0.5002 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2257 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2257 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2257 - acc: 0.5001 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2257 - acc: 0.4994 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2257 - acc: 0.4995 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2257 - acc: 0.5000 - val_loss: 1.2257 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5001 - val_loss: 1.2259 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4999 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4994 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5000 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4996 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4992 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4996 - val_loss: 1.2257 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5005 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4992 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 19/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4998 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 20/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4995 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 21/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5003 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 22/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5005 - val_loss: 1.2257 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 23/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 24/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4996 - val_loss: 1.2257 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4996 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 26/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5005 - val_loss: 1.2257 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 27/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4998 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 28/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5003 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 29/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5000 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 30/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4996 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 31/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 32/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5000 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 33/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5002 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 34/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4999 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 35/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 36/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5000 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 37/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5001 - val_loss: 1.2257 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 38/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4998 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 39/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4999 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 40/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 41/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5000 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 42/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4995 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 43/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5003 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 44/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5001 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 45/100\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.5006 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 46/100\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "1202978/1202978 - 4s - loss: 1.2256 - acc: 0.4997 - val_loss: 1.2256 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 00046: early stopping\n",
      "Unfolding iteration 1\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_50 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_52 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_53 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_54 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_55 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-1_Iteration-0\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-2_Iteration-0\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 6s - loss: 0.8870 - acc: 0.4851 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4485 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4562 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4367 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4339 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4186 - val_loss: 0.8867 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4408 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4331 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4528 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4404 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4342 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4160 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4372 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4383 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4336 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4413 - val_loss: 0.8867 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4218 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "1662570/1662570 - 6s - loss: 0.8867 - acc: 0.4406 - val_loss: 0.8867 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 00018: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5003 - val_loss: 1.2261 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4994 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4995 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5003 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4995 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5002 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5004 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5000 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4997 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5000 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5000 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4995 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5001 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5000 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5000 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.5009 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4994 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "1202978/1202978 - 4s - loss: 1.2260 - acc: 0.4992 - val_loss: 1.2260 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 00018: early stopping\n",
      "Unfolding iteration 2\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_56 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_57 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_58 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_59 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_60 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_61 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_62 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_63 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-1_Iteration-1\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-2_Iteration-1\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.5053 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4848 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4869 - val_loss: 0.8851 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4740 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.5082 - val_loss: 0.8850 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4846 - val_loss: 0.8851 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4797 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.5014 - val_loss: 0.8851 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4767 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4922 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.5120 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4997 - val_loss: 0.8851 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4712 - val_loss: 0.8851 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4785 - val_loss: 0.8851 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 15/100\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "1662570/1662570 - 6s - loss: 0.8851 - acc: 0.4854 - val_loss: 0.8851 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 00015: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202978/1202978 - 5s - loss: 1.2225 - acc: 0.5005 - val_loss: 1.2225 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 5s - loss: 1.2225 - acc: 0.4995 - val_loss: 1.2225 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.5003 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4995 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4996 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4997 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 5s - loss: 1.2225 - acc: 0.4998 - val_loss: 1.2225 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 5s - loss: 1.2225 - acc: 0.5005 - val_loss: 1.2226 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4996 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.5003 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4996 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4999 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4996 - val_loss: 1.2226 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4994 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "1202978/1202978 - 4s - loss: 1.2225 - acc: 0.4991 - val_loss: 1.2225 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 00015: early stopping\n",
      "Unfolding iteration 3\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_64 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_65 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_66 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_67 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_68 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_69 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_70 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_71 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-1_Iteration-2\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-2_Iteration-2\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 6s - loss: 0.8855 - acc: 0.4780 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4719 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4745 - val_loss: 0.8856 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4797 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4545 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4744 - val_loss: 0.8856 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4881 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4753 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4742 - val_loss: 0.8856 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4786 - val_loss: 0.8856 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4637 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4755 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4704 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4624 - val_loss: 0.8856 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "1662570/1662570 - 6s - loss: 0.8856 - acc: 0.4720 - val_loss: 0.8856 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 00015: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 5s - loss: 1.2245 - acc: 0.4998 - val_loss: 1.2245 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 5s - loss: 1.2245 - acc: 0.5000 - val_loss: 1.2245 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 5s - loss: 1.2245 - acc: 0.4998 - val_loss: 1.2245 - val_acc: 0.5000 - 5s/epoch - 4us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4999 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4997 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5002 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4999 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5002 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5008 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5006 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5000 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5003 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4999 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5007 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4999 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4992 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.4998 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5002 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 19/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5002 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 20/100\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5000 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 21/100\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "1202978/1202978 - 4s - loss: 1.2245 - acc: 0.5001 - val_loss: 1.2245 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 00021: early stopping\n",
      "Unfolding iteration 4\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_72 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_73 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_74 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_75 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 100)               200       \n",
      "                                                                 \n",
      " activation_76 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_77 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_78 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,602\n",
      "Trainable params: 20,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Step 1 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-1_Iteration-3\n",
      "Step 2 - loading weights from ptktraining/ThrustUnifold_patience-10_batchsize-500_trw-2_Step-2_Iteration-3\n",
      "Train on 1662570 samples, validate on 415642 samples\n",
      "Epoch 1/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4934 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4937 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4906 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4822 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4962 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4981 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5149 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4925 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4990 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5161 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4943 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4943 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4957 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5079 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5059 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 16/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5078 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4892 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4971 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 19/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4927 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 3us/sample\n",
      "Epoch 20/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5101 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 21/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.5027 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n",
      "Epoch 22/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4779 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 23/100\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4854 - val_loss: 0.8849 - val_acc: 0.3618 - 6s/epoch - 3us/sample\n",
      "Epoch 24/100\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "1662570/1662570 - 6s - loss: 0.8849 - acc: 0.4980 - val_loss: 0.8849 - val_acc: 0.6382 - 6s/epoch - 4us/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00024: early stopping\n",
      "Train on 1202978 samples, validate on 300744 samples\n",
      "Epoch 1/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.4994 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 2/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5000 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 3/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5008 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 4/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5002 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 5/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5005 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 6/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.4990 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 7/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5005 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 8/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5002 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 9/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5007 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 10/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.4997 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 11/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5001 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 12/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.4995 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 13/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5000 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 14/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5005 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 15/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.4990 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 16/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5004 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 17/100\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.4997 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 3us/sample\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "1202978/1202978 - 4s - loss: 1.2235 - acc: 0.5005 - val_loss: 1.2235 - val_acc: 0.5000 - 4s/epoch - 4us/sample\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "for trw_ind in [0, -2]:\n",
    "\n",
    "    name = 'ThrustUnifold_patience-10_batchsize-500_trw{}'.format(trw_ind)\n",
    "    fitargs = {'epochs': 100, 'batch_size': 500, 'verbose': 2}\n",
    "\n",
    "    # iterative unfolding\n",
    "    for i in range(itnum):\n",
    "        print('Unfolding iteration', i)\n",
    "\n",
    "        model_det_fp = 'ptktraining/{}_Step-1_Iteration-{}'.format(name, '{}')\n",
    "        model_mc_fp = 'ptktraining/{}_Step-2_Iteration-{}'.format(name, '{}')\n",
    "\n",
    "        # define detector reweighting model\n",
    "        model_det = ef.archs.DNN(input_dim=len(obs_multifold), dense_sizes=model_layer_sizes, patience=10,\n",
    "                                 #filepath=model_det_fp.format(i) + '_Epoch-{epoch}',\n",
    "                                 optimizer=Adam(lr=0.0005))\n",
    "\n",
    "        # define particle reweighting model\n",
    "        model_mc = ef.archs.DNN(input_dim=len(obs_multifold), dense_sizes=model_layer_sizes, patience=10,\n",
    "                                #filepath=model_mc_fp.format(i) + '_Epoch-{epoch}',\n",
    "                                optimizer=Adam(lr=0.0005))\n",
    "\n",
    "        # load wieghts if not iteration 0\n",
    "        if i > 0:\n",
    "            print('Step 1 - loading weights from', model_det_fp.format(i-1))\n",
    "            model_det.load_weights(model_det_fp.format(i-1))\n",
    "\n",
    "            print('Step 2 - loading weights from', model_mc_fp.format(i-1))\n",
    "            model_mc.load_weights(model_mc_fp.format(i-1))\n",
    "\n",
    "        # step 1: reweight sim to look like data\n",
    "        w = np.concatenate((wdata, ws[-1]))\n",
    "        w_train, w_val = w[perm_det[:-nval_det]], w[perm_det[-nval_det:]]\n",
    "        rw = reweight(X_det_train, Y_det_train, w_train, model_det, model_det_fp.format(i),\n",
    "                      fitargs, val_data=(X_det_val, Y_det_val, w_val))[invperm_det]\n",
    "        ws.append(rw[len(wdata):])\n",
    "\n",
    "        # step 2: reweight the prior to the learned weighting\n",
    "        w = np.concatenate((ws[-1], ws[trw_ind]))\n",
    "        w_train, w_val = w[perm_gen[:-nval_gen]], w[perm_gen[-nval_gen:]]\n",
    "        rw = reweight(X_gen_train, Y_gen_train, w_train, model_mc, model_mc_fp.format(i),\n",
    "                      fitargs, val_data=(X_gen_val, Y_gen_val, w_val))[invperm_gen]\n",
    "        ws.append(rw[len(ws[-1]):])\n",
    "\n",
    "        # save the weights if specified\n",
    "        np.save(name, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Unfolding Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRUST,THRUST LOW,THRUST HIGH,(1/SIG)*D(SIG)/DTHRUST,stat +,stat -,sys_1 +,sys_1 -,sys_2 +,sys_2 -\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(aleph_path, 'HEPData-ins636645-v1-Table_54.csv'), 'r') as f:\n",
    "    \n",
    "    vals = []\n",
    "    for row in f:\n",
    "        if row.startswith('#'):\n",
    "            continue\n",
    "            \n",
    "        if row.startswith('T'):\n",
    "            print(row.strip())\n",
    "        else:\n",
    "            vals.append(row.strip().split(','))\n",
    "            \n",
    "hepdata = np.asarray(vals, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aleph_bins = 1 - np.append(1.0, hepdata[::-1,1])\n",
    "aleph_binwidths = aleph_bins[1:] - aleph_bins[:-1]\n",
    "aleph_midbins = (aleph_bins[1:] + aleph_bins[:-1])/2\n",
    "aleph_thrust = hepdata[::-1,3]\n",
    "aleph_thrust_errs = np.linalg.norm(hepdata[::-1,[-1,-3,-5]], axis=1)\n",
    "assert np.all(aleph_bins[1:] == 1 - hepdata[::-1,1]) and np.all(aleph_bins[:-1] == 1 - hepdata[::-1,2])\n",
    "\n",
    "plt.errorbar(aleph_midbins, aleph_thrust, xerr=0.005, yerr=aleph_thrust_errs, label='ALEPH Eur.Phys.J. C (2004) 457-486',\n",
    "             color='green', **modplot.style('errorbar'))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Thrust $1 - T$')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend(loc='lower left', frameon=False)\n",
    "plt.show()\n",
    "\n",
    "aleph_log_bins = np.log(aleph_bins + np.exp(-8))\n",
    "aleph_log_midbins = (aleph_log_bins[1:] + aleph_log_bins[:-1])/2\n",
    "aleph_log_binwidths = aleph_log_bins[1:] - aleph_log_bins[:-1]\n",
    "aleph_log_thrust = aleph_thrust * 0.01 / aleph_log_binwidths\n",
    "aleph_log_thrust_errs = aleph_thrust_errs * 0.01 / aleph_log_binwidths\n",
    "\n",
    "plt.errorbar(aleph_log_midbins, aleph_log_thrust, color='green', label='ALEPH Measurement',\n",
    "             xerr=aleph_log_binwidths/2, yerr=aleph_log_thrust_errs, **modplot.style('errorbar'))\n",
    "plt.yscale('log')\n",
    "plt.xlim(-8, 0)\n",
    "plt.ylim(10**-4, 1)\n",
    "plt.xlabel(r'Thrust $\\ln(1 - T)$')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend(loc='lower left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "latex was not able to process the following string:\nb'lp'\n\nHere is the full report generated by latex:\nThis is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) (preloaded format=latex)\n restricted \\write18 enabled.\nentering extended mode\n\n(/Users/anthonybadea/.matplotlib/tex.cache/3bbc0f8d536770a62891b7cc8788c317.tex\nLaTeX2e <2018-12-01>\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/article.cls\nDocument Class: article 2018/09/03 v1.4i Standard LaTeX document class\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/size10.clo))\n\n! LaTeX Error: File `type1cm.sty' not found.\n\nType X to quit or <RETURN> to proceed,\nor enter new name. (Default extension: sty)\n\nEnter file name: \n! Emergency stop.\n<read *> \n         \nl.5 \\usepackage\n               {type1ec}^^M\nNo pages of output.\nTranscript written on 3bbc0f8d536770a62891b7cc8788c317.log.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             report = subprocess.check_output(command,\n\u001b[0m\u001b[1;32m    276\u001b[0m                                              \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    412\u001b[0m                **kwargs).stdout\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    513\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['latex', '-interaction=nonstopmode', '--halt-on-error', '/Users/anthonybadea/.matplotlib/tex.cache/3bbc0f8d536770a62891b7cc8788c317.tex']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c45e23153b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m               line_0=r'$\\textsc{Preliminary}$') # 'Preliminary')\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmodplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ThrustWithUniFold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m252\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;31m#fig.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ALEPH/ALEPHOmnifold/omnifold/modplot.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(fig, name, add_watermark, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_watermark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../plots/bare/{}.pdf'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_watermark\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mwatermark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}.pdf'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2309\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2191\u001b[0m                            else suppress())\n\u001b[1;32m   2192\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m                     bbox_inches = self.figure.get_tightbbox(\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0m\u001b[1;32m   1166\u001b[0m                                                                 renderer)\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1092\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1093\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1092\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1093\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Full vertical extent of font, including ascenders and descenders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             ismath=\"TeX\" if self.get_usetex() else False)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/backends/_backend_pdf_ps.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtexmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 s, fontsize, renderer=self)\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mtexfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 self._run_checked_subprocess(\n\u001b[0m\u001b[1;32m    310\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[1;32m    311\u001b[0m                      texfile], tex)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 'found'.format(command[0])) from exc\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    284\u001b[0m                 \u001b[0;34m'{prog} was not able to process the following string:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;34m'{tex!r}\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: latex was not able to process the following string:\nb'lp'\n\nHere is the full report generated by latex:\nThis is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) (preloaded format=latex)\n restricted \\write18 enabled.\nentering extended mode\n\n(/Users/anthonybadea/.matplotlib/tex.cache/3bbc0f8d536770a62891b7cc8788c317.tex\nLaTeX2e <2018-12-01>\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/article.cls\nDocument Class: article 2018/09/03 v1.4i Standard LaTeX document class\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/size10.clo))\n\n! LaTeX Error: File `type1cm.sty' not found.\n\nType X to quit or <RETURN> to proceed,\nor enter new name. (Default extension: sty)\n\nEnter file name: \n! Emergency stop.\n<read *> \n         \nl.5 \\usepackage\n               {type1ec}^^M\nNo pages of output.\nTranscript written on 3bbc0f8d536770a62891b7cc8788c317.log.\n\n\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "latex was not able to process the following string:\nb'lp'\n\nHere is the full report generated by latex:\nThis is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) (preloaded format=latex)\n restricted \\write18 enabled.\nentering extended mode\n\n(/Users/anthonybadea/.matplotlib/tex.cache/3bbc0f8d536770a62891b7cc8788c317.tex\nLaTeX2e <2018-12-01>\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/article.cls\nDocument Class: article 2018/09/03 v1.4i Standard LaTeX document class\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/size10.clo))\n\n! LaTeX Error: File `type1cm.sty' not found.\n\nType X to quit or <RETURN> to proceed,\nor enter new name. (Default extension: sty)\n\nEnter file name: \n! Emergency stop.\n<read *> \n         \nl.5 \\usepackage\n               {type1ec}^^M\nNo pages of output.\nTranscript written on 3bbc0f8d536770a62891b7cc8788c317.log.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             report = subprocess.check_output(command,\n\u001b[0m\u001b[1;32m    276\u001b[0m                                              \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    412\u001b[0m                **kwargs).stdout\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    513\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['latex', '-interaction=nonstopmode', '--halt-on-error', '/Users/anthonybadea/.matplotlib/tex.cache/3bbc0f8d536770a62891b7cc8788c317.tex']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2191\u001b[0m                            else suppress())\n\u001b[1;32m   2192\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m                     bbox_inches = self.figure.get_tightbbox(\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0m\u001b[1;32m   1166\u001b[0m                                                                 renderer)\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1092\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1093\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1092\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1093\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Full vertical extent of font, including ascenders and descenders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             ismath=\"TeX\" if self.get_usetex() else False)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mtexmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    228\u001b[0m                 s, fontsize, renderer=self)\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mtexfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 self._run_checked_subprocess(\n\u001b[0m\u001b[1;32m    310\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[1;32m    311\u001b[0m                      texfile], tex)\n",
      "\u001b[0;32m~/anaconda3/envs/rpv_multijet/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex)\u001b[0m\n\u001b[1;32m    281\u001b[0m                 'found'.format(command[0])) from exc\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    284\u001b[0m                 \u001b[0;34m'{prog} was not able to process the following string:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;34m'{tex!r}\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: latex was not able to process the following string:\nb'lp'\n\nHere is the full report generated by latex:\nThis is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019) (preloaded format=latex)\n restricted \\write18 enabled.\nentering extended mode\n\n(/Users/anthonybadea/.matplotlib/tex.cache/3bbc0f8d536770a62891b7cc8788c317.tex\nLaTeX2e <2018-12-01>\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/article.cls\nDocument Class: article 2018/09/03 v1.4i Standard LaTeX document class\n(/usr/local/texlive/2019basic/texmf-dist/tex/latex/base/size10.clo))\n\n! LaTeX Error: File `type1cm.sty' not found.\n\nType X to quit or <RETURN> to proceed,\nor enter new name. (Default extension: sty)\n\nEnter file name: \n! Emergency stop.\n<read *> \n         \nl.5 \\usepackage\n               {type1ec}^^M\nNo pages of output.\nTranscript written on 3bbc0f8d536770a62891b7cc8788c317.log.\n\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['text.usetex'] = True\n",
    "obkey = 'Thrust'\n",
    "ob = obs[obkey]\n",
    "corrs = np.ones(genbhist.shape)\n",
    "corrs = genbhist/(genhist + 10**-50)\n",
    "\n",
    "# convert obs to numpy\n",
    "ob[\"genobs\"] = np.array(ob[\"genobs\"])\n",
    "ob[\"dataobs\"] = np.array(ob[\"dataobs\"])\n",
    "ob[\"simobs\"] = np.array(ob[\"simobs\"])\n",
    "\n",
    "omnifold_ws = np.load('ThrustUnifold_patience-10_batchsize-500_trw0.npy')\n",
    "\n",
    "fig, [ax0, ax1] = modplot.axes(**ob, gridspec_update={'height_ratios': (3, 1)})\n",
    "if ob.get('yscale') is not None:\n",
    "    ax0.set_yscale(ob['yscale'])\n",
    "\n",
    "# plot the \"data\" histogram of the observable\n",
    "ax0.hist(ob['dataobs'], bins=ob['bins_det'], color='black', label='ALEPH Raw 1994 Data', **hist_style)\n",
    "\n",
    "# plot the \"sim\" histogram of the observable\n",
    "ax0.hist(ob['simobs'], bins=ob['bins_det'], color='orange', label='Pythia 6 + Geant 3 Sim.', **hist_style)\n",
    "\n",
    "# plot the \"gen\" histogram of the observable\n",
    "ax0.plot(ob['midbins_mc'], ob['genobs_hist'], **gen_style)\n",
    "\n",
    "# plot the IBU distribution\n",
    "ibu_hist = ob['ibu_phis'][itnum]*corrs\n",
    "ax0.errorbar(ob['midbins_mc'], ibu_hist, xerr=ob['binwidth_mc']/2, yerr=ob['ibu_phi_unc']*corrs,\n",
    "             color='gray', label='IBU {} + stat.'.format(ob['symbol']), **modplot.style('errorbar'))\n",
    "\n",
    "# plot the UniFold distribution\n",
    "omnifold_hist, omnifold_errs = modplot.calc_hist(ob['genobs'], bins=ob['bins_mc'], \n",
    "                                                 weights=omnifold_ws[-5], density=True)[:2]\n",
    "ax0.errorbar(ob['midbins_mc'], omnifold_hist, xerr=ob['binwidth_mc']/2, yerr=omnifold_errs*corrs,\n",
    "             color='tab:red', label='UniFold {} + stat.'.format(ob['symbol']), **modplot.style('errorbar'))\n",
    "\n",
    "# plot the ALEPH measurement\n",
    "ax0.errorbar(aleph_midbins, aleph_thrust, color='green', label='ALEPH E.P.J. C (2004)',\n",
    "             xerr=aleph_binwidths/2, yerr=aleph_thrust_errs, **modplot.style('errorbar'))\n",
    "\n",
    "# RATIO PLOTS\n",
    "ax1.plot([np.min(ob['midbins_mc']), np.max(ob['midbins_mc'])], [1, 1], '--', color='blue', lw=0.75)\n",
    "genobs_hist, genobs_errs = modplot.calc_hist(ob['genobs'], bins=ob['bins_mc'], density=True)[:2]\n",
    "\n",
    "# central value calculations\n",
    "gen_hist_aleph_bins = modplot.calc_hist(ob['genobs'], bins=aleph_bins, density=False)[0]\n",
    "gen_hist_aleph_bins /= aleph_binwidths * np.sum(gen_hist_aleph_bins)\n",
    "aleph_ratio = aleph_thrust/(gen_hist_aleph_bins + 1e-50)\n",
    "ibu_ratio = ibu_hist/(genobs_hist + 10**-50)\n",
    "omnifold_ratio = omnifold_hist*corrs/(genobs_hist + 1e-50)\n",
    "\n",
    "# error calculations\n",
    "aleph_unc_ratio = aleph_thrust_errs/(gen_hist_aleph_bins + 10**-50)\n",
    "ibu_unc_ratio = ob['ibu_phi_unc']*corrs/(genobs_hist + 10**-50)\n",
    "omnifold_unc_ratio = omnifold_errs*corrs/(genobs_hist + 10**-50)\n",
    "\n",
    "# plot ALEPH ratio\n",
    "ax1.errorbar(aleph_midbins, aleph_ratio, xerr=aleph_binwidths/2, yerr=aleph_unc_ratio,\n",
    "             color='green', **modplot.style('errorbar', lw=1))\n",
    "\n",
    "# data ratio\n",
    "data_hist, data_errs = modplot.calc_hist(ob['dataobs'], bins=ob['bins_det'], density=True)[:2]\n",
    "ax1.errorbar(ob['midbins_det'], data_hist/(genobs_hist + 10**-50), xerr=ob['binwidth_det']/2,\n",
    "             yerr=data_errs/(genobs_hist + 10**-50), color='black', **modplot.style('errorbar'))\n",
    "\n",
    "# sim ratio\n",
    "sim_hist, sim_errs = modplot.calc_hist(ob['simobs'], bins=ob['bins_det'], density=True)[:2]\n",
    "ax1.errorbar(ob['midbins_det'], sim_hist/(genobs_hist + 10**-50), xerr=ob['binwidth_det']/2,\n",
    "             yerr=sim_errs/(genobs_hist + 10**-50), color='orange', **modplot.style('errorbar'))\n",
    "\n",
    "# plot IBU ratio\n",
    "ax1.errorbar(ob['midbins_mc'], ibu_ratio, xerr=ob['binwidth_mc']/2, yerr=ibu_unc_ratio,\n",
    "             color=ibu_style['color'], **modplot.style('errorbar'))\n",
    "\n",
    "# plot OmniFold ratio\n",
    "ax1.errorbar(ob['midbins_mc'], omnifold_ratio, xerr=ob['binwidth_mc']/2, yerr=omnifold_unc_ratio,\n",
    "             color='tab:red', **modplot.style('errorbar'))\n",
    "\n",
    "ax1.fill_between(ob['midbins_mc'], 1 - genobs_errs, 1 + genobs_errs, facecolor='blue', zorder=-2)\n",
    "\n",
    "loc, ncol = ob.get('legend_loc', 'upper right'), ob.get('legend_ncol', 2)\n",
    "order = [5, 1, 2, 0, 3, 4]\n",
    "modplot.legend(ax=ax0, frameon=False, loc=(0.05, 0.05), order=order, fontsize=8, markerscale=0)\n",
    "\n",
    "modplot.stamp(0.6, 0.875, ax=ax0, textops_update={'color': 'red', 'fontsize': 12},\n",
    "              line_0=r'$\\textsc{Preliminary}$') # 'Preliminary')\n",
    "\n",
    "modplot.save(fig, 'ThrustWithUniFold', tx=44, ty=252)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
